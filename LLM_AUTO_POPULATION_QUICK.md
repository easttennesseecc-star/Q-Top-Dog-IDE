# LLM Auto-Population Quick Reference\n\n## What Changed?\nThe LLM pool now **automatically selects the best available LLM** without user action.\n\n## Priority Ranking (What Gets Selected First)\n```\n150 pts → GitHub Copilot (best)\n120 pts → Gemini, ChatGPT, Grok, Claude\n65 pts  → Ollama CLI\n60 pts  → Llama CLI\n30 pts  → Other running processes\n20 pts  → Local model files\n```\n\n## How It Works\n1. App loads → fetches `/llm_pool/best?count=3`\n2. Backend returns top 3 LLMs sorted by priority\n3. Frontend checks localStorage\n4. If no LLM selected → **auto-select #1** and save it\n5. Display green section showing what was selected\n6. User can click any other option to change\n\n## UI Changes\n- Green \"✨ Auto-Selected Best Options\" section at top\n- Shows 3 best LLMs in grid layout\n- Displays priority score for each\n- Click to select different option\n\n## Audit Logging\nAll auto-selections logged:\n```json\n{\n  \"action\": \"auto_select\",\n  \"who\": \"system\",\n  \"priority_score\": 150,\n  \"model\": { ... }\n}\n```\n\n## API Endpoints\n\n### Get All LLMs (with priority scores)\n```\nGET /llm_pool\nResponse: { available: [...], excluded: [...] }\n```\n\n### Get Best N LLMs (auto-population)\n```\nGET /llm_pool/best?count=3\nResponse: { best: [...], count: 3, note: \"...\" }\n```\n\n## Testing\n\n### Verify Backend\n```bash\ncurl http://localhost:8000/llm_pool/best?count=3\n```\n\n### Verify Frontend\n1. Open LLM tab\n2. Look for green \"Auto-Selected Best Options\" section\n3. Check browser console:\n   ```javascript\n   localStorage.getItem('selectedLLM')  // Should have LLM data\n   localStorage.getItem('llmAutoSelected')  // Should be 'true'\n   ```\n\n## Code Locations\n\n**Backend**:\n- `backend/llm_pool.py` - `get_llm_priority_score()`, `get_best_llms_for_operations()`\n- `backend/main.py` - `/llm_pool/best` endpoint\n\n**Frontend**:\n- `frontend/src/components/LLMPoolPanel.tsx` - `loadBest()`, auto-select logic, UI\n\n## Customization\n\n### Change Priority Weights\nEdit `llm_pool.py` `get_llm_priority_score()` function:\n```python\nif \"copilot\" in name:\n    score += 50  # Change this number (higher = more priority)\n```\n\n### Change Default Count\nIn `LLMPoolPanel.tsx`:\n```typescript\nconst res = await fetch('/llm_pool/best?count=5');  // Was 3\n```\n\n## Common Scenarios\n\n### Scenario 1: Copilot Available\n→ Copilot auto-selected (150 pts)\n\n### Scenario 2: Only Cloud Services\n→ First detected (Gemini/ChatGPT/Grok) auto-selected (120 pts)\n\n### Scenario 3: Only Local\n→ Ollama auto-selected if available, else local model files (20 pts)\n\n### Scenario 4: No LLMs Found\n→ Green section hidden, user shown \"No available assistants\"\n\n## Production Checklist\n\n✅ Priority scoring implemented\n✅ `/llm_pool/best` endpoint working\n✅ Frontend auto-selection working\n✅ Audit logging for auto-selections\n✅ User can still manually override\n✅ Error handling and fallbacks\n✅ Build successful\n✅ localStorage persistence\n✅ Documented\n\n## Features\n\n✨ Zero configuration needed\n✨ Intelligent LLM ranking\n✨ User controllable\n✨ Fully auditable\n✨ Transparent (shows priority scores)\n✨ Production ready\n