# ü§ñ LLM Learning System - Complete Setup Guide

## üìã What You Now Have

A complete, production-ready backend system that enables your coding LLM to:
- **Learn from builds** by accessing logs, errors, and success patterns
- **Understand your project** through codebase structure and source file analysis  
- **Submit intelligence** with confidence scores and actionable recommendations
- **Improve continuously** by analyzing patterns across hundreds of builds

## üìÅ New Files Created

### Backend Core
| File | Size | Purpose |
|------|------|---------|
| `backend/llm_client.py` | 300+ LOC | Python client library for LLM integration |
| `backend/llm_agent_example.py` | 400+ LOC | Full working example agent implementation |
| `backend/test_llm_learning.py` | 150+ LOC | Test script to validate all endpoints |

### Documentation  
| File | Purpose |
|------|---------|
| `backend/LLM_LEARNING_GUIDE.md` | Complete developer guide with examples |
| `backend/LLM_LEARNING_IMPLEMENTATION.md` | Architecture decisions and implementation details |
| `README.md` (updated) | Added LLM learning section to main docs |

### Backend Updates
- `backend/main.py` - Added 4 new `/llm/learning/*` endpoints (250+ lines)

## üöÄ Quick Start (3 Steps)

### 1. Start Backend
```bash
cd C:\Quellum-topdog-ide
.venv\Scripts\Activate.ps1
python -m uvicorn backend.main:app --reload --host 127.0.0.1 --port 8000
```

### 2. Test Connectivity
```bash
cd backend
python test_llm_learning.py
```

### 3. Start Your LLM Agent
```bash
python llm_agent_example.py
```

## üîå API Endpoints

All endpoints live at `http://127.0.0.1:8000/llm/learning/`:

### GET `/builds` - Fetch build history
```python
from backend.llm_client import LLMClient
client = LLMClient()
builds = client.get_builds(limit=20, skip=0)
# Returns: BuildSummary with list of builds
```

### GET `/build/{id}` - Detailed build analysis
```python
build_data = client.get_build("uuid-123")
# Returns: {status, log, log_summary with errors/warnings}
```

### GET `/codebase` - Project structure & sources
```python
codebase = client.get_codebase()
# Returns: {file_tree, key_config_files, backend_files, structure_summary}
```

### POST `/report` - Submit analysis
```python
client.submit_report(
    build_id="uuid-123",
    type="failure_analysis",
    analysis="Your findings here",
    recommendations=["Fix X", "Check Y"],
    confidence=0.85
)
```

## üí° Usage Examples

### Example 1: Simple Analysis
```python
from backend.llm_client import LLMClient

client = LLMClient()

# Get failed builds
builds = client.get_builds(limit=50)
failed = [b for b in builds.builds if b.status == "failed"]

# Analyze first failure
if failed:
    data = client.get_build(failed[0].id)
    errors = data["log_summary"]["errors"]
    print(f"Build failed with {len(errors)} errors")
    for error in errors[:3]:
        print(f"  - {error}")
```

### Example 2: Pattern Learning
```python
# See llm_agent_example.py for full implementation
from llm_agent_example import QIDECodingAgent

agent = QIDECodingAgent()
agent.continuous_learning_loop()  # Runs until stopped (Ctrl+C)
```

### Example 3: With Your LLM (OpenAI)
```python
import openai
from backend.llm_client import LLMClient

client = LLMClient()
openai.api_key = "your-key"

# Get failed build
builds = client.get_builds(limit=20)
build_data = client.get_build(builds.builds[0].id)

# Ask LLM to analyze
errors = build_data["log_summary"]["errors"]
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a build failure analyst."},
        {"role": "user", "content": f"Analyze these build errors: {errors}"}
    ]
)

# Submit findings
client.submit_report(
    build_id=builds.builds[0].id,
    type="failure_analysis",
    analysis=response.choices[0].message.content,
    confidence=0.85
)
```

## üìä Endpoint Response Examples

### GET /llm/learning/builds
```json
{
  "status": "ok",
  "total": 150,
  "builds": [
    {
      "id": "uuid-123",
      "status": "success",
      "log": "... full output ..."
    }
  ],
  "skip": 0,
  "limit": 20
}
```

### GET /llm/learning/build/{id}
```json
{
  "status": "ok",
  "build": {
    "id": "uuid-123",
    "status": "failed",
    "log": "... output ...",
    "log_summary": {
      "total_lines": 245,
      "error_count": 3,
      "warning_count": 7,
      "errors": ["ModuleNotFoundError: No module named 'foo'", ...],
      "warnings": ["DeprecationWarning: ...", ...]
    }
  }
}
```

### GET /llm/learning/codebase
```json
{
  "status": "ok",
  "codebase": {
    "workspace_root": "/path/to/project",
    "file_tree": { "type": "directory", "children": [...] },
    "key_config_files": {
      "package.json": "{ ... }",
      "pyproject.toml": "..."
    },
    "backend_files": [
      {"name": "main.py", "path": "backend/main.py", "lines": 415}
    ],
    "structure_summary": {
      "has_backend": true,
      "has_frontend": true,
      "has_tests": true
    }
  }
}
```

## üéØ Common Use Cases

### Use Case 1: Automatic Failure Triage
```python
# When a build fails, automatically classify the issue
failures = {
    "import_error": "Missing dependencies",
    "test_failure": "Broken tests",
    "timeout": "Performance issue",
    "syntax_error": "Code error",
    "access_error": "Permission issue"
}
```
See: `llm_agent_example.py` - `analyze_build_failure()` method

### Use Case 2: Project Health Dashboard
```python
# Generate a health score for the project
health_metrics = {
    "success_rate": 0.87,
    "failure_rate": 0.13,
    "avg_build_time": 120,
    "test_coverage": 0.85,
    "last_failure": "2 hours ago"
}
```

### Use Case 3: Predictive Build Optimization
```python
# Identify patterns that lead to failures and predict issues before they happen
patterns = [
    "Dependency updates cause import errors",
    "Large file changes increase test timeout",
    "Test suite grows slower than code changes"
]
```

### Use Case 4: AI Code Review
```python
# Analyze build failures in context of recent code changes
codebase = client.get_codebase()
backend_files = codebase["backend_files"]
# Feed into your LLM for intelligent review
```

## üîê Security & Authentication

### Development (No Auth)
```python
# Works immediately in local dev
client = LLMClient()
```

### Production (With OAuth)
```python
# With OAuth session from Sign In flow
session_id = "oauth-session-from-login"
client = LLMClient(session_id=session_id)
```

The backend validates session_id if provided, allowing gradual deployment.

## üìà Performance & Scaling

### Current Architecture
- **Storage**: In-memory `BUILD_STORE` dictionary
- **Persistence**: Build data available for current session
- **Scale**: ~1000 builds max before memory concerns

### For Production
1. Replace `BUILD_STORE` with database (PostgreSQL, MongoDB)
2. Add build archival (keep recent, compress old)
3. Implement caching for codebase structure
4. Use background tasks for large log processing
5. Add rate limiting on endpoints

See `LLM_LEARNING_IMPLEMENTATION.md` for details.

## üß™ Testing

### Run Tests
```bash
# Test basic connectivity
python backend/test_llm_learning.py

# Run pytest on LLMClient (when tests available)
python -m pytest backend/test_llm_learning.py -v
```

### Manual Testing
```python
# Quick sanity check
from backend.llm_client import LLMClient

client = LLMClient()
try:
    builds = client.get_builds(limit=1)
    assert builds.total >= 0
    print("‚úì Backend connectivity OK")
except Exception as e:
    print(f"‚úó Backend error: {e}")
```

## üìö Documentation Files

Read in this order:

1. **This file** - Overview and quick start
2. `backend/LLM_LEARNING_GUIDE.md` - Complete developer guide
3. `backend/llm_client.py` - Client API reference
4. `backend/llm_agent_example.py` - Example agent implementation
5. `README.md` - Integrated into main docs

## üö¶ Next Steps

### Immediate (Today)
- [ ] Review this guide
- [ ] Run `python backend/test_llm_learning.py`
- [ ] Read `backend/LLM_LEARNING_GUIDE.md`
- [ ] Run example agent: `python backend/llm_agent_example.py`

### Short Term (This Week)
- [ ] Integrate with your preferred LLM (GPT-4, Claude, Ollama, etc.)
- [ ] Customize pattern detection in `llm_agent_example.py`
- [ ] Deploy agent as background service
- [ ] Create frontend UI panel showing AI recommendations

### Medium Term (This Month)
- [ ] Build dashboard showing build health and AI insights
- [ ] Implement database storage for build history
- [ ] Add security audit patterns (dependencies, vulnerabilities)
- [ ] Integrate with GitHub for additional context
- [ ] Fine-tune LLM with domain-specific examples

### Long Term (Ongoing)
- [ ] Scale to multiple agents (one per domain: security, performance, etc.)
- [ ] Implement feedback loops (agent learns from its recommendations' outcomes)
- [ ] Add predictive models for build failure prevention
- [ ] Export metrics and reports
- [ ] Share learnings across projects

## ü§ù Integration Checklist

- [ ] Backend running (`uvicorn backend.main:app --reload`)
- [ ] `LLMClient` imports and connects
- [ ] Can fetch recent builds
- [ ] Can analyze codebase structure
- [ ] Can submit reports
- [ ] Example agent runs without errors
- [ ] Tests pass (`python backend/test_llm_learning.py`)

## ‚ùì FAQ

**Q: Can I use this with GPT-4/Claude?**
A: Yes! See examples in `LLM_LEARNING_GUIDE.md`. Just import their SDK and use the data from LLMClient.

**Q: Is authentication required?**
A: No for local dev. Pass `session_id` in production to enforce auth via OAuth integration.

**Q: How much build history is kept?**
A: Currently in-memory for session. In production, implement database storage.

**Q: Can I run multiple agents?**
A: Yes! Each agent is independent and can run with different parameters/poll intervals.

**Q: How do I deploy this?**
A: Agent can run as systemd service, Docker container, or background process in your IDE.

## üêõ Troubleshooting

| Issue | Solution |
|-------|----------|
| "Connection refused" | Start backend: `python -m uvicorn backend.main:app --reload` |
| "No builds found" | Trigger a build first via `/build/run` or UI |
| Import errors | Install dependencies: `pip install requests` |
| Timeout errors | Increase timeout or use pagination with smaller limits |
| Session not found | Don't need session for dev; add valid OAuth ID for auth |

## üìû Support Resources

- Full guide: `backend/LLM_LEARNING_GUIDE.md`
- Example code: `backend/llm_agent_example.py`
- Test script: `backend/test_llm_learning.py`
- API source: `backend/main.py` (search for `/llm/learning/`)
- README docs: `README.md` section "LLM Learning"

---

## üéâ You're Ready!

Your Q-IDE backend is now equipped with a complete LLM learning system. Your coding LLM can:

‚úÖ Access full build history and logs  
‚úÖ Analyze project structure and dependencies  
‚úÖ Detect failure patterns automatically  
‚úÖ Submit intelligent recommendations  
‚úÖ Learn and improve continuously  

Start with `python backend/test_llm_learning.py` to verify connectivity, then explore the examples in the documentation.

**Questions?** Refer to `backend/LLM_LEARNING_GUIDE.md` or check the example implementation in `backend/llm_agent_example.py`.

Happy learning! üöÄ
